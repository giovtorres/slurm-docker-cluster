services:
  mysql:
    image: mariadb:12
    hostname: mysql
    container_name: mysql
    environment:
      MYSQL_RANDOM_ROOT_PASSWORD: "yes"
      MYSQL_DATABASE: ${MYSQL_DATABASE:-slurm_acct_db}
      MYSQL_USER: ${MYSQL_USER:-slurm}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-password}
    volumes:
      - var_lib_mysql:/var/lib/mysql
    networks:
      - slurm-network
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  slurmdbd:
    image: slurm-docker-cluster:${SLURM_VERSION:-25.05.3}
    build:
      context: .
      args:
        SLURM_VERSION: ${SLURM_VERSION:-25.05.3}
      cache_from:
        - slurm-docker-cluster:${SLURM_VERSION:-25.05.3}
    command: ["slurmdbd"]
    container_name: slurmdbd
    hostname: slurmdbd
    environment:
      MYSQL_USER: ${MYSQL_USER:-slurm}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-password}
    volumes:
      - etc_munge:/etc/munge
      - etc_slurm:/etc/slurm
      - var_log_slurm:/var/log/slurm
    expose:
      - "6819"
    depends_on:
      mysql:
        condition: service_healthy
    networks:
      - slurm-network
    healthcheck:
      test: ["CMD-SHELL", "pidof slurmdbd"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

  slurmctld:
    image: slurm-docker-cluster:${SLURM_VERSION:-25.05.3}
    command: ["slurmctld"]
    container_name: slurmctld
    hostname: slurmctld
    privileged: true
    working_dir: /data
    volumes:
      - etc_munge:/etc/munge:z
      - etc_slurm:/etc/slurm:z
      - slurm_jobdir:/data:z
      - var_log_slurm:/var/log/slurm:z
    expose:
      - "6817"
    depends_on:
      slurmdbd:
        condition: service_healthy
    networks:
      - slurm-network
    healthcheck:
      test: ["CMD-SHELL", "scontrol ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

  slurmrestd:
    image: slurm-docker-cluster:${SLURM_VERSION:-25.05.3}
    command: ["slurmrestd"]
    container_name: slurmrestd
    hostname: slurmrestd
    privileged: true
    volumes:
      - etc_munge:/etc/munge
      - etc_slurm:/etc/slurm
      - var_log_slurm:/var/log/slurm
    ports:
      - "6820:6820"
    expose:
      - "6820"
    depends_on:
      slurmctld:
        condition: service_healthy
    networks:
      - slurm-network
    healthcheck:
      test: ["CMD-SHELL", "test -S /var/run/slurmrestd/slurmrestd.socket"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

  c1:
    image: slurm-docker-cluster:${SLURM_VERSION:-25.05.3}
    command: ["slurmd"]
    container_name: c1
    hostname: c1
    working_dir: /data
    privileged: true
    volumes:
      - etc_munge:/etc/munge
      - etc_slurm:/etc/slurm
      - slurm_jobdir:/data
      - var_log_slurm:/var/log/slurm
    expose:
      - "6818"
    depends_on:
      slurmctld:
        condition: service_healthy
    networks:
      - slurm-network
    healthcheck:
      test: ["CMD-SHELL", "pidof slurmd"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

  c2:
    image: slurm-docker-cluster:${SLURM_VERSION:-25.05.3}
    command: ["slurmd"]
    container_name: c2
    hostname: c2
    working_dir: /data
    privileged: true
    volumes:
      - etc_munge:/etc/munge
      - etc_slurm:/etc/slurm
      - slurm_jobdir:/data
      - var_log_slurm:/var/log/slurm
    expose:
      - "6818"
    depends_on:
      slurmctld:
        condition: service_healthy
    networks:
      - slurm-network
    healthcheck:
      test: ["CMD-SHELL", "pidof slurmd"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

  g1:
    image: slurm-docker-cluster-gpu:${SLURM_VERSION:-25.05.3}
    build:
      context: .
      dockerfile: Dockerfile.gpu
      args:
        SLURM_VERSION: ${SLURM_VERSION:-25.05.3}
    # runtime: nvidia
    command: ["slurmd"]
    container_name: g1
    hostname: g1
    # privileged: true
    # gpus: "all"
    # gpus:
    #   - device: "0"  # Expose only GPU 0
    environment:
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      NVIDIA_VISIBLE_DEVICES: 0  # Expose only GPU 0
    # Give Slurm just enough to manage cgroup v2 without systemd
    cap_add:
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
      - seccomp:unconfined
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']  # Explicitly specify GPU 0
              capabilities: [gpu]
    volumes:
      - /sys/fs/cgroup:/sys/fs/cgroup:rw
      - etc_munge:/etc/munge
      - etc_slurm:/etc/slurm
      - slurm_jobdir:/data
      - var_log_slurm:/var/log/slurm
    expose:
      - "6818"
    depends_on:
      slurmctld:
        condition: service_healthy
    networks:
      - slurm-network
    healthcheck:
      test: ["CMD-SHELL", "pidof slurmd"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

volumes:
  etc_munge:
  etc_slurm:
  slurm_jobdir:
  var_lib_mysql:
  var_log_slurm:

networks:
  slurm-network:
    driver: bridge
